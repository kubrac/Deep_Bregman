{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import tqdm\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments(object):\n",
    "    dataset = # possible data sets to try: 'mnist', 'svhn_cropped', 'cifar10', 'cifar100',\n",
    "    lossChoice = # 'all_triplet', 'hard_triplet', 'semihard_triplet', 'contrastive'\n",
    "    save_path = # enter a path to save\n",
    "    excel_path = ''\n",
    "    runBreg = # True      \n",
    "    trial_index = # enter an id for the experiment\n",
    "    \n",
    "    def add_args(self, args_dict):\n",
    "        [setattr(self, key, args_dict[key]) for key in args_dict]\n",
    "        \n",
    "    def get_args(self):\n",
    "        arg_list = {var: getattr(self, var) for var in dir(self)\n",
    "                    if not callable(getattr(self, var))\n",
    "                    and not var.startswith(\"__\")}\n",
    "        return arg_list\n",
    "    def print_args(self):\n",
    "        arg_dict = self.get_args()\n",
    "        print_str = ''\n",
    "        for arg in arg_dict:\n",
    "            print_str += arg + ' = ' + str(arg_dict[arg]) + '\\n'\n",
    "        return print_str\n",
    "    \n",
    "    @classmethod\n",
    "    def init_csv(self, name):\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "        path = os.path.join(self.save_path, self.dataset + '_' + name + '.csv')\n",
    "        print(path)\n",
    "        headers = ['model_number', 'lossChoice',  \n",
    "                   'test_acc_breg', 'test_auc_breg',\n",
    "                  'test_avp_breg', 'test_acc_euc',\n",
    "                  'test_auc_euc', 'test_avp_euc'] + list(self.get_args(self).keys()) \n",
    "        with open(path, 'w') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "            csv_writer.writerow(headers)\n",
    "        self.excel_path = path\n",
    "        return path\n",
    "\n",
    "    def init_excel(self, name):\n",
    "        if not os.path.isfile(self.save_path + name):\n",
    "            wb = Workbook()\n",
    "            sheet1 = wb.add_sheet('Sheet 1')\n",
    "            for i, h in enumerate(self.headers):\n",
    "                sheet1.write(i, 0, self.headers)\n",
    "            wb.save(self.save_path + name)\n",
    "            self.excel_path = self.save_path + name\n",
    "            return self.excel_path\n",
    "            \n",
    "    def addrow_to_excel(self, val_list):\n",
    "        #with open(excel_path) as f:\n",
    "        #    reader = csv.reader(f)\n",
    "        data = [self.trial_index, self.lossChoice] + val_list + list(self.get_args().values())  #, '%.3f'%(knn_train), \n",
    "        with open(self.excel_path, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n",
    "\n",
    "arguments = Arguments()\n",
    "excel_path = Arguments.init_csv('results')\n",
    "print(excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_divergences(embed):\n",
    "\n",
    "    max_out = tf.math.argmax(embed, 1, output_type=tf.dtypes.int32)\n",
    "    one_to_n = tf.range(tf.shape(embed)[0], dtype=tf.dtypes.int32)\n",
    "    max_indices = tf.transpose(tf.stack([one_to_n, max_out]))\n",
    "    max_values = tf.gather_nd(embed, max_indices)\n",
    "    max_values_repeated = tf.transpose(tf.reshape(tf.tile(max_values, [tf.shape(embed)[0]]), [tf.shape(embed)[0], tf.shape(embed)[0]]))\n",
    "    repeated_max_out = tf.tile(max_out, [tf.shape(embed)[0]])\n",
    "    repeated_one_to_n = tf.tile(one_to_n, [tf.shape(embed)[0]])\n",
    "    mat_rotn = tf.reshape(tf.transpose(tf.reshape(repeated_one_to_n, [tf.shape(embed)[0], tf.shape(embed)[0]])), [-1])\n",
    "    new_max_indices = tf.transpose(tf.stack([mat_rotn, repeated_max_out]))\n",
    "    new_max_values = tf.gather_nd(embed, new_max_indices)\n",
    "    reshaped_new_max_values = tf.reshape(new_max_values, [tf.shape(embed)[0], tf.shape(embed)[0]])\n",
    "    div_matrix = tf.maximum(tf.subtract(max_values_repeated, reshaped_new_max_values), 0.0)  \n",
    "    \n",
    "#    #for differentiability, this version uses softmax instead of argmax\n",
    "#    sftmx = tf.nn.softmax(tf.multiply(1.0, embed))\n",
    "#    ES = tf.linalg.matmul(embed, sftmx, transpose_b=True)\n",
    "#    one_vec = tf.reshape(tf.ones([tf.shape(embed)[0]]), [1, tf.shape(embed)[0]])\n",
    "#    diag_ES = tf.reshape(tf.linalg.diag_part(ES), [1, tf.shape(embed)[0]])\n",
    "#    max_outputs = tf.linalg.matmul(diag_ES, one_vec, transpose_a=True)\n",
    "#    div_matrix = tf.maximum(tf.subtract(max_outputs, ES), 0.0)\n",
    "    \n",
    "    return div_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "    if not squared:\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = tf.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False, breg=False):\n",
    "\n",
    "    if breg:\n",
    "        pairwise_dist = _pairwise_divergences(embeddings)\n",
    "    else:\n",
    "        pairwise_dist = _pairwise_distances(embeddings, squared=True)  \n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "    \n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False, breg=False):\n",
    "\n",
    "    if breg:\n",
    "        pairwise_dist = _pairwise_divergences(embeddings)\n",
    "    else:\n",
    "        pairwise_dist = _pairwise_distances(embeddings, squared=True)  \n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss\n",
    "  \n",
    "def contrastive_loss(labels, embeddings, margin, breg=False):\n",
    "\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_j = tf.to_float(i_equal_j)\n",
    "    i_equal_j = tf.reshape(i_equal_j, [tf.shape(embeddings)[0], tf.shape(embeddings)[0]])\n",
    "\n",
    "    if breg:\n",
    "        distances = _pairwise_divergences(embeddings)\n",
    "    else:\n",
    "        distances = _pairwise_distances(embeddings, squared=True)    \n",
    "\n",
    "    sim_term = tf.math.multiply(i_equal_j, distances)\n",
    "    dissim_term = tf.multiply(tf.subtract(1.0, i_equal_j), tf.maximum(tf.subtract(margin,distances), 0))\n",
    "    #dissim_term = tf.multiply(tf.subtract(1.0, i_equal_j),  tf.pow(tf.maximum(tf.subtract(margin, sqrt_distances), 0), 2))\n",
    "    cont_loss = tf.reduce_mean(tf.add(sim_term, dissim_term))\n",
    "    return cont_loss\n",
    "\n",
    "def _masked_maximum(data, mask, dim=1):\n",
    "\n",
    "    axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = tf.math.reduce_max(\n",
    "        tf.math.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "  \n",
    "def _masked_minimum(data, mask, dim=1):\n",
    "\n",
    "    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = tf.math.reduce_min(\n",
    "        tf.math.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "  \n",
    "def triplet_semihard_loss(y_true, y_pred, margin=1.0, breg=False):\n",
    "\n",
    "    labels, embeddings = y_true, y_pred\n",
    "    lshape = tf.shape(labels)\n",
    "    labels = tf.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    if breg:\n",
    "        pdist_matrix = _pairwise_divergences(embeddings)\n",
    "    else:\n",
    "        pdist_matrix = _pairwise_distances(embeddings, squared=True) \n",
    "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
    "    adjacency_not = tf.math.logical_not(adjacency)\n",
    "\n",
    "    batch_size = tf.size(labels)\n",
    "\n",
    "    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = tf.math.logical_and(\n",
    "        tf.tile(adjacency_not, [batch_size, 1]),\n",
    "        tf.math.greater(pdist_matrix_tile,\n",
    "                        tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = tf.reshape(\n",
    "        tf.math.greater(\n",
    "            tf.math.reduce_sum(\n",
    "                tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = tf.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n",
    "\n",
    "    negatives_outside = tf.reshape(\n",
    "        _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = tf.transpose(negatives_outside)\n",
    "    negatives_inside = tf.tile(\n",
    "        _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = tf.where(mask_final, negatives_outside,\n",
    "                                   negatives_inside)\n",
    "\n",
    "    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = tf.cast(\n",
    "        adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
    "            tf.ones([batch_size]))\n",
    "\n",
    "    num_positives = tf.math.reduce_sum(mask_positives)\n",
    "\n",
    "    triplet_loss = tf.math.truediv(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(lossChoice, runBreg, margin, pretrain, y, k, out):\n",
    "    if lossChoice == 'contrastive':\n",
    "        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, k, dtype=tf.float32), logits=out))\n",
    "        loss2 = contrastive_loss(y, out, margin, breg=runBreg)\n",
    "        loss = pretrain*loss1 + (1-pretrain)*loss2\n",
    "\n",
    "    if lossChoice == 'all_triplet':\n",
    "        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, k, dtype=tf.float32), logits=out))\n",
    "        loss2, fpt = batch_all_triplet_loss(y, out, margin, squared=True, breg=runBreg)\n",
    "        loss = pretrain*loss1 + (1-pretrain)*loss2\n",
    "\n",
    "    if lossChoice == 'hard_triplet':\n",
    "        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, k, dtype=tf.float32), logits=out))\n",
    "        loss2 = batch_hard_triplet_loss(y, out, margin, squared=True, breg=runBreg)\n",
    "        loss = pretrain*loss1 + (1-pretrain)*loss2\n",
    "\n",
    "    if lossChoice == 'semihard_triplet':\n",
    "        loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, k, dtype=tf.float32), logits=out))\n",
    "        loss2 = triplet_semihard_loss(y, out, margin, breg=runBreg)\n",
    "        loss = pretrain*loss1 + (1-pretrain)*loss2\n",
    "        \n",
    "    return loss1, loss2, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_euc_test_np(embed_test, test_y, K):\n",
    "    knn_x_train, knn_x_test, knn_y_train, knn_y_test = train_test_split(embed_test, test_y, test_size=0.33, random_state=10)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=K, metric='euclidean')\n",
    "    knn.fit(knn_x_train, knn_y_train)\n",
    "    pred_prob = knn.predict_proba(knn_x_test)\n",
    "    y_pred = knn.predict(knn_x_test)\n",
    "\n",
    "    one = time.time()\n",
    "    knn_y_test_onehot = np.eye(10)[knn_y_test]\n",
    "    acc = accuracy_score(knn_y_test, y_pred)\n",
    "    avp = average_precision_score(knn_y_test_onehot, pred_prob)\n",
    "    auc_ovo = roc_auc_score(knn_y_test, pred_prob, multi_class='ovo')\n",
    "    \n",
    "    return acc, avp, auc_ovo #accuracy_score(knn_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_breg_test_np(test_X, test_Y, K):\n",
    "    \n",
    "    def bregman_twopoints(x,y):\n",
    "        term1 = np.max(x)\n",
    "        term2 = x[np.argmax(y)]\n",
    "        return term1 - term2\n",
    "\n",
    "    knn_x_train, knn_x_test, knn_y_train, knn_y_test = train_test_split(test_X, test_Y, test_size=0.33, random_state=10)\n",
    "    knn = KNeighborsClassifier(n_neighbors=K, metric=bregman_twopoints)\n",
    "\n",
    "    knn.fit(knn_x_train, knn_y_train)\n",
    "    pred_prob = knn.predict_proba(knn_x_test)\n",
    "    y_pred = knn.predict(knn_x_test)\n",
    "\n",
    "    knn_y_test_onehot = np.eye(10)[knn_y_test]\n",
    "    acc = accuracy_score(knn_y_test, y_pred)\n",
    "    avp = average_precision_score(knn_y_test_onehot, pred_prob)\n",
    "    auc_ovo = roc_auc_score(knn_y_test, pred_prob, multi_class='ovo')\n",
    "    \n",
    "    return acc, avp, auc_ovo #accuracy_score(knn_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_model(conv_filters, height, width,input_channels,batch_size, k, training_ph, x, reuse=None):\n",
    "    with tf.variable_scope(\"euc_model\", reuse=reuse):\n",
    "        new_height = height\n",
    "        new_width = width\n",
    "        layer1 = create_new_conv_layer(x, input_channels, conv_filters[0], \n",
    "                                       [3, 3], [2, 2], training_bool=training_ph, \n",
    "                                       name='layer1', doPool=False)\n",
    "        layer2 = create_new_conv_layer(layer1, conv_filters[0], conv_filters[1], \n",
    "                                       [3, 3], [2, 2], training_bool=training_ph, \n",
    "                                       name='layer2', doPool=True)\n",
    "        new_height = new_height / 2\n",
    "        new_width = new_width / 2\n",
    "        layer3 = create_new_conv_layer(layer2, conv_filters[1], conv_filters[2], \n",
    "                                       [3, 3], [2, 2], training_bool=training_ph,\n",
    "                                       name='layer3', doPool=False)\n",
    "        layer4 = create_new_conv_layer(layer3, conv_filters[2], conv_filters[3],\n",
    "                                       [3, 3], [2, 2], training_bool=training_ph, \n",
    "                                       name='layer4', doPool=True)\n",
    "        new_height = int(new_height / 2)\n",
    "        new_width = int(new_width / 2)\n",
    "\n",
    "        flattened = tf.reshape(layer4, [-1, new_height * new_width * conv_filters[3]])\n",
    "\n",
    "        wd1 = tf.get_variable('wd1', [new_width * new_height * conv_filters[3], 1000], initializer = tf.initializers.glorot_uniform())\n",
    "        bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "        dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "        dense_layer1 = tf.layers.batch_normalization(dense_layer1, training=training_ph)\n",
    "        dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "  #wd2 = tf.Variable(tf.truncated_normal([1000, k], stddev=0.03), name='wd2')\n",
    "        wd2 = tf.get_variable('wd2', [1000, k], initializer = tf.initializers.glorot_uniform())\n",
    "        bd2 = tf.Variable(tf.truncated_normal([k], stddev=0.01), name='bd2')\n",
    "        dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "        out = dense_layer2 \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, training_bool, name, act_func, useBias= True, doPool=True, batchNorm=True):\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
    "                      num_filters]\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    if useBias:\n",
    "        bias = tf.Variable(tf.truncated_normal([num_filters], stddev=0.01), name=name+'_b')\n",
    "        out_layer += bias\n",
    "\n",
    "    if batchNorm:\n",
    "        out_layer = tf.layers.batch_normalization(out_layer, training=training_bool)\n",
    "\n",
    "    if act_func == 'relu':\n",
    "        out_layer = tf.nn.relu(out_layer)\n",
    "\n",
    "    if doPool:\n",
    "        ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "        strides = [1, 2, 2, 1]\n",
    "        out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
    "                                 padding='SAME')\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breg_model(model_args, height, width,input_channels,batch_size, k, training_ph, x, reuse=None):   \n",
    "    conv_filters = [2 ** (int(n)) for n in model_args['conv_layers']['conv_filter_nums']] \n",
    "    conv_doPools = [n for n in model_args['conv_layers']['conv_doPools_list']]     \n",
    "    conv_batchNorms = [n for n in model_args['conv_layers']['conv_batchNorms_list']]  \n",
    "    conv_kernels = [int(2*n - 1) for n in model_args['conv_layers']['conv_kernels_list']]  \n",
    "    conv_pools = [int(n) for n in model_args['conv_layers']['conv_pools_list']]   \n",
    "    act_func = model_args['activation']\n",
    "    dense_hiddens = int(model_args['dense_layers']) \n",
    "    breg_batchNorm = model_args['breg_batchNorm'] \n",
    "    useBiases = model_args['conv_layers']['useBiases'] \n",
    "    \n",
    "    with tf.variable_scope(\"breg_model\", reuse=reuse):\n",
    "        new_height = height\n",
    "        new_width = width\n",
    "        conv_out = create_new_conv_layer(x, input_channels, conv_filters[0],\n",
    "                                        [conv_kernels[0], conv_kernels[0]], \n",
    "                                        [conv_pools[0], conv_pools[0]], \n",
    "                                        training_bool=training_ph,\n",
    "                                        name='layer1', \n",
    "                                        act_func = act_func, \n",
    "                                        useBias = useBiases[0], \n",
    "                                        doPool=conv_doPools[0],\n",
    "                                        batchNorm = conv_batchNorms[0])\n",
    "        \n",
    "        if conv_doPools[0] == True:\n",
    "            new_height = int(new_height / 2)\n",
    "            new_width = int(new_width / 2)\n",
    "        \n",
    "        for i in range(len(conv_filters)-1): #(num_layers - 1):\n",
    "            conv_out = create_new_conv_layer(conv_out, conv_filters[i], conv_filters[i+1],\n",
    "                                       [conv_kernels[i], conv_kernels[i]], \n",
    "                                       [conv_pools[0], conv_pools[0]],\n",
    "                                       training_bool=training_ph,\n",
    "                                       name='layer%d' %(i), \n",
    "                                       act_func = act_func, \n",
    "                                       useBias = useBiases[i], \n",
    "                                        doPool=conv_doPools[i],\n",
    "                                        batchNorm = conv_batchNorms[i])\n",
    "            if conv_doPools[i] == True:\n",
    "                new_height = int(new_height / 2)\n",
    "                new_width = int(new_width / 2)\n",
    "        \n",
    "        flattened = tf.reshape(conv_out, [-1, new_height * new_width * conv_filters[-1]])\n",
    "\n",
    "        rmat = tf.truncated_normal([new_height * new_width * conv_filters[-1], dense_hiddens], \n",
    "                                   stddev = 0.03)\n",
    "        rmat2 = tf.truncated_normal([dense_hiddens], stddev=0.01)\n",
    "        rmat3 = tf.truncated_normal([1], stddev=0.01)\n",
    "        rmat4 = tf.truncated_normal([dense_hiddens, 1], stddev=0.03)\n",
    "\n",
    "        wd_layer = 'wd1_0'\n",
    "        wd = tf.Variable(rmat, name=wd_layer)\n",
    "        bd_layer = 'bd1_0'\n",
    "        bd = tf.Variable(rmat2, name=bd_layer)\n",
    "\n",
    "        dense_layer1 = tf.matmul(flattened, wd) + bd\n",
    "        \n",
    "        if breg_batchNorm:\n",
    "            dense_layer1 = tf.layers.batch_normalization(dense_layer1, training=training_ph)\n",
    "            \n",
    "        dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "        wd_layer2 = 'wd2_0'\n",
    "        wd2 = tf.Variable(rmat4, name=wd_layer2)\n",
    "        bd_layer2 = 'bd2_0'\n",
    "        bd2 = tf.Variable(rmat3, name=bd_layer2)\n",
    "\n",
    "        dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "\n",
    "        out = dense_layer2\n",
    "\n",
    "        for i in range(k-1):\n",
    "            wd_layer = 'wd1_' + str(i+1)\n",
    "            wd = tf.Variable(rmat, name=wd_layer)\n",
    "            bd_layer = 'bd1_' + str(i+1)\n",
    "            bd = tf.Variable(rmat2, name=bd_layer)\n",
    "\n",
    "            dense_layer1 = tf.matmul(flattened, wd) + bd\n",
    "            \n",
    "            if breg_batchNorm:\n",
    "                dense_layer1 = tf.layers.batch_normalization(dense_layer1, training=training_ph)\n",
    "                \n",
    "            dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "            wd_layer2 = 'wd2_' + str(i+1)\n",
    "            wd2 = tf.Variable(rmat4, name=wd_layer2)\n",
    "            bd_layer2 = 'bd2_' + str(i+1)\n",
    "            bd2 = tf.Variable(rmat3, name=bd_layer2)\n",
    "\n",
    "            dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "            out = tf.concat([out, dense_layer2], axis=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "def train(args, train_x, train_y, data_test, k, n_train, n_test):\n",
    "    model_param_dict = args.model_params\n",
    "    optimizer_name = args.optimizer\n",
    "    print(model_param_dict['conv_layers']['conv_filter_nums'])\n",
    "    \n",
    "    save_path = args.save_path\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    cur_path = save_path + str(args.trial_index) + '/'\n",
    "    if not os.path.exists(cur_path):\n",
    "        os.makedirs(cur_path)\n",
    "    \n",
    "    if args.runBreg:\n",
    "        name = 'breg_'\n",
    "    else:\n",
    "        name = 'euc_'\n",
    "\n",
    "    with open(cur_path + name + \"arguments.txt\", \"w\") as f: \n",
    "        f.write(args.print_args())\n",
    "    \n",
    "    K = int(args.K)\n",
    "    height = train_x.shape[1]\n",
    "    width = train_x.shape[2]\n",
    "    input_channels = train_x.shape[3]\n",
    "    batch_size = int(args.batch_size)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, height, width, input_channels])\n",
    "    y = tf.placeholder(tf.int32, [None,])\n",
    "    pretrain = tf.placeholder(tf.float32)\n",
    "    training_ph = tf.placeholder(tf.bool)\n",
    "    test_x_ph = tf.placeholder(tf.float32, [None, height, width, input_channels])\n",
    "    test_y_ph = tf.placeholder(tf.int64, [None,])\n",
    "    \n",
    "    if args.runBreg:\n",
    "        out = breg_model(model_param_dict, height, width,input_channels,\n",
    "                     batch_size, k, training_ph, x)\n",
    "    else:    \n",
    "        out = breg_model(model_param_dict, height, width,input_channels,\n",
    "                     batch_size, k, training_ph, x)\n",
    "    \n",
    "    if args.is_normalize == True:\n",
    "        out = tf.nn.l2_normalize(out, axis=1)\n",
    "    \n",
    "    _, _, loss = get_loss(args.lossChoice, args.runBreg, args.margin, pretrain, y, k, out)\n",
    "\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        if optimizer_name == 'adam':\n",
    "            optimiser = tf.train.AdamOptimizer(learning_rate=args.learning_rate).minimize(loss)\n",
    "        elif optimizer_name == 'sgd': \n",
    "            optimiser = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate).minimize(loss)\n",
    "        else:\n",
    "            optimiser = tf.train.RMSPropOptimizer(learning_rate=args.learning_rate).minimize(loss)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    output = out\n",
    "        \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(n_train / batch_size)\n",
    "        loss_list = []\n",
    "        \n",
    "        saver =tf.train.Saver()\n",
    "        print(args.epochs, 'epoch number chosen')\n",
    "        for epoch in range(int(args.epochs)):\n",
    "            epoch_stime = time.time()\n",
    "            avg_cost = 0\n",
    "            rp = np.random.permutation(train_y.shape[0])\n",
    "            train_x = train_x[rp,:,:]\n",
    "            train_y = train_y[rp]\n",
    "            for i in range(total_batch):\n",
    "                batch_x = train_x[i*batch_size:(i+1)*batch_size-1,:,:].copy()\n",
    "                batch_y = train_y[i*batch_size:(i+1)*batch_size-1].copy()\n",
    "                if epoch > int(args.pretrain_epoch):\n",
    "                    _, c = sess.run([optimiser, loss], \n",
    "                                    feed_dict={x: batch_x, y: batch_y, \n",
    "                                               training_ph:True, pretrain: 0.0 })\n",
    "                else:\n",
    "                    _, c = sess.run([optimiser, loss], \n",
    "                                    feed_dict={x: batch_x, y: batch_y, \n",
    "                                               training_ph:True, pretrain: 1.0 })\n",
    "                avg_cost += c / total_batch\n",
    "            loss_list.append(avg_cost)\n",
    "            epoch_etime = time.time()\n",
    "            if epoch == 0:\n",
    "                print('time per epoch: %.2f' %(epoch_etime - epoch_stime))\n",
    "\n",
    "        #saver.save(sess, cur_path + name + 'trained_model', global_step=int(args.epochs))            \n",
    "        print(\"\\nTraining complete!\")\n",
    "        \n",
    "        data_test = data_test.batch(n_test)\n",
    "\n",
    "        iterator_test = tf.data.make_one_shot_iterator(data_test)\n",
    "        next_element = iterator_test.get_next()\n",
    "        test_x = next_element[\"image\"]\n",
    "        test_y = next_element[\"label\"]\n",
    "        test_x = tfds.as_numpy(test_x) / 255\n",
    "        test_x = 2*(test_x - 0.5)\n",
    "        test_y = tfds.as_numpy(test_y)\n",
    "\n",
    "        if args.runBreg:\n",
    "            file_prefix = args.save_path + args.dataset + '-' + args.lossChoice + '-bregman-'\n",
    "        else:\n",
    "            file_prefix = args.save_path + args.dataset + '-' + args.lossChoice + '-euclidean-'\n",
    "        \n",
    "        np.savetxt(cur_path + name + 'loss.txt', loss_list, delimiter='\\n')\n",
    "        print('start testing')\n",
    "        test_stime = time.time()\n",
    "        embed_test = output.eval(feed_dict={x: test_x, training_ph:False, pretrain: 0.0, })\n",
    "        \n",
    "        if args.runBreg:\n",
    "            breg_test_acc, breg_test_avp, breg_test_auc = knn_breg_test_np(embed_test, \n",
    "                                                                       test_y, K)\n",
    "        else:\n",
    "            breg_test_acc, breg_test_avp, breg_test_auc = knn_euc_test_np(embed_test, \n",
    "                                                                       test_y, K)\n",
    "                    \n",
    "        val_list = [breg_test_acc, breg_test_auc, breg_test_avp] \n",
    "  \n",
    "        return (1 - breg_test_acc),val_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(args=arguments):\n",
    "    data, info = tfds.load(name=args.dataset, with_info=True)\n",
    "    data_train = data['train']\n",
    "    data_test = data['test']\n",
    "\n",
    "    k = info.features['label'].num_classes\n",
    "    n_train = info.splits['train'].num_examples\n",
    "    n_test = info.splits['test'].num_examples\n",
    "    data_train = data_train.batch(n_train)\n",
    "\n",
    "    iterator =  tf.data.make_one_shot_iterator(data_train)\n",
    "    element = iterator.get_next()\n",
    "    train_x, train_Y = element[\"image\"], element[\"label\"]\n",
    "    train_x = tfds.as_numpy(train_x).astype(np.float32) / 255.0\n",
    "\n",
    "    train_x = 2.0*(train_x - 0.5)\n",
    "    train_y = tfds.as_numpy(train_Y)\n",
    "    print(np.max(train_y))\n",
    "\n",
    "    train_acc, metrics_list = train(args, train_x, train_y, data_test, k, n_train, n_test)\n",
    "    return train_acc, metrics_list\n",
    "\n",
    "def objective(params):\n",
    "    global trial_idx\n",
    "    global excel_path\n",
    "    tf.reset_default_graph()\n",
    "    args = Arguments()\n",
    "    \n",
    "    args.add_args(params)\n",
    "    args.trial_index = trial_idx\n",
    "    args.excel_path = excel_path\n",
    "    res, eval_list_breg = train_test(args)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    args.runBreg = False\n",
    "    _, eval_list_euc = train_test(args)\n",
    "\n",
    "    args.addrow_to_excel(eval_list_breg + eval_list_euc)\n",
    "    trial_idx += 1\n",
    "    return res #{'test_acc': res, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def conv_filter_nums(num_layers):\n",
    "    conv_filter_list = []\n",
    "    for l in range(num_layers):\n",
    "        f_name = 'filter_%d%d' % (num_layers, (l+1))\n",
    "        conv_filter_list.append(hp.quniform(f_name, 3, 7, 2)) \n",
    "    return conv_filter_list\n",
    "\n",
    "def conv_doPools(num_layers):\n",
    "    conv_doPools_list = [True] * num_layers\n",
    "    for l in range(min(num_layers, 3)):\n",
    "        f_name = 'doPools_%d%d' % (num_layers, (l+1))\n",
    "        conv_doPools_list[-l-1] = hp.choice(f_name, [True, False])\n",
    "    return conv_doPools_list\n",
    "\n",
    "def conv_batchNorms(num_layers):\n",
    "    conv_batchNorms_list = []\n",
    "    for l in range(num_layers):\n",
    "        f_name = 'batchNorm_%d%d' % (num_layers, (l+1))\n",
    "        conv_batchNorms_list.append(hp.choice(f_name, [True, False]))\n",
    "    return conv_batchNorms_list\n",
    "\n",
    "def conv_kernels(num_layers):\n",
    "    conv_kernels_list = []\n",
    "    for l in range(num_layers):\n",
    "        f_name = 'kernel_%d%d' % (num_layers, (l+1))\n",
    "        conv_kernels_list.append(hp.quniform(f_name, 1, 4, 2)) \n",
    "    return conv_kernels_list\n",
    "\n",
    "def conv_pools(num_layers):\n",
    "    conv_pools_list = []\n",
    "    for l in range(num_layers):\n",
    "        f_name = 'pool_%d%d' % (num_layers, (l+1))\n",
    "        conv_pools_list.append(hp.quniform(f_name, 1, 2, 2)) \n",
    "    return conv_pools_list\n",
    "\n",
    "def useBias(num_layers):\n",
    "    useBias_list = []\n",
    "    for l in range(num_layers):\n",
    "        f_name = 'bias_%d%d' % (num_layers, (l+1))\n",
    "        useBias_list.append(hp.choice(f_name, [True, False])) \n",
    "    return useBias_list\n",
    "\n",
    "#provide f space\n",
    "trials = Trials()\n",
    "\n",
    "ss = time.time()\n",
    "\n",
    "best = fmin(objective,\n",
    "    space=fspace,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "pickle.dump(trials, open(args.save_path + \"/hyperopt_results.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
